{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c64ef2",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6346fad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/atexier/ipeo_venv/lib/python3.7/site-packages (1.13.0)\n",
      "Requirement already satisfied: torchvision in /home/atexier/ipeo_venv/lib/python3.7/site-packages (0.14.0)\n",
      "Requirement already satisfied: torchtext in /home/atexier/ipeo_venv/lib/python3.7/site-packages (0.14.0)\n",
      "Requirement already satisfied: pytorch_lightning in /home/atexier/ipeo_venv/lib/python3.7/site-packages (1.8.2)\n",
      "Requirement already satisfied: tensorboard in /home/atexier/ipeo_venv/lib/python3.7/site-packages (2.11.0)\n",
      "Requirement already satisfied: matplotlib in /home/atexier/ipeo_venv/lib/python3.7/site-packages (3.5.3)\n",
      "Requirement already satisfied: tqdm in /home/atexier/ipeo_venv/lib/python3.7/site-packages (4.64.1)\n",
      "Requirement already satisfied: wget in /home/atexier/ipeo_venv/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: typing-extensions in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: wheel in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.38.4)\n",
      "Requirement already satisfied: setuptools in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (41.2.0)\n",
      "Requirement already satisfied: requests in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: numpy in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: fsspec[http]>2021.06.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pytorch_lightning) (2022.11.0)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\n",
      "Requirement already satisfied: torchmetrics>=0.7.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pytorch_lightning) (0.10.3)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pytorch_lightning) (21.3)\n",
      "Requirement already satisfied: lightning-utilities==0.3.* in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pytorch_lightning) (0.3.0)\n",
      "Requirement already satisfied: fire in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from lightning-utilities==0.3.*->pytorch_lightning) (0.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (3.4.1)\n",
      "Requirement already satisfied: protobuf<4,>=3.9.2 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (3.20.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (2.14.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (1.3.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (1.50.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (2.2.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from tensorboard) (0.6.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard) (5.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from requests->torchvision) (2.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: termcolor in /home/atexier/ipeo_venv/lib/python3.7/site-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning) (2.1.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!pip install torch torchvision torchtext pytorch_lightning tensorboard matplotlib tqdm wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7378ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  7 12:38:36 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                  Off |\r\n",
      "| N/A   34C    P0    24W / 250W |      4MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b61cc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import numpy as np \n",
    "import csv\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3b781",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b678862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import zipfile\n",
    "if not os.path.exists(os.getcwd()+\"/ipeo_data\"):\n",
    "    with zipfile.ZipFile(os.getcwd()+\"/ipeo_data.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e394c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = open(os.getcwd()+\"/ipeo_data/splits/train.csv\")\n",
    "train_csv  = csv.reader(train_csv)\n",
    "test_csv = open(os.getcwd()+\"/ipeo_data/splits/test.csv\")\n",
    "test_csv  = csv.reader(test_csv)\n",
    "val_csv = open(os.getcwd()+\"/ipeo_data/splits/val.csv\")\n",
    "val_csv  = csv.reader(val_csv)\n",
    "### CAREFUL need to remove the \"25595_11025_label\" from the validation dataset. Empty image ?\n",
    "### remove direcly in the val excel fil (row = 629)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbaa75dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7357"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_csv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3872783",
   "metadata": {},
   "source": [
    "## Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bf4a2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "\"\"\"CLASS rocks (1), scree (2), sparse rocks (3), water (4), glacier and permanent snow (5), forest(6), sparse forest(7),\n",
    "grasslands and others (8).\"\"\"\n",
    "\n",
    "class Alpine(Dataset):\n",
    "\n",
    "    # mapping between label class names and indices\n",
    "    LABEL_CLASSES = {\n",
    "      'rocks': \t\t  7,\n",
    "      'scree': \t\t\t    6,\n",
    "      'sparse_rocks': \t  5,\n",
    "      'water': \t\t\t\t      3,\n",
    "      'glacier_and_permanent_snow': \t\t\t    4,\n",
    "      'forest': \t\t\t    1,\n",
    "      'sparse_forest':   2,\n",
    "      'grasslands_and_others': \t\t\t\t    0,\n",
    "      \n",
    "    }\n",
    "\n",
    "    def __init__(self, transforms=None, split='train',frac=1.0):\n",
    "        self.transforms = transforms\n",
    "        #only for label_image\n",
    "        transforms_label = T.Compose([\n",
    "        T.ToTensor()\n",
    "        ])\n",
    "        self.transforms_label = transforms_label\n",
    "        \n",
    "        # prepare data\n",
    "        self.data = []                                  # list of tuples of (image path, label class)\n",
    "        # get images with correct index according to dataset split\n",
    "        if split=='train':\n",
    "            data_csv = open(os.getcwd()+\"/ipeo_data/splits/train.csv\")\n",
    "            data_csv  = list(csv.reader(data_csv))\n",
    "            length = int(frac*len(data_csv))\n",
    "        if split=='test':\n",
    "            data_csv = open(os.getcwd()+\"/ipeo_data/splits/test.csv\")\n",
    "            data_csv  = list(csv.reader(data_csv))\n",
    "            length = int(frac*len(data_csv))\n",
    "        if split=='val':\n",
    "            data_csv = open(os.getcwd()+\"/ipeo_data/splits/val.csv\")\n",
    "            data_csv  = list(csv.reader(data_csv))\n",
    "            length = int(frac*len(data_csv))\n",
    "        \n",
    "        i=0\n",
    "        print(length)\n",
    "        \n",
    "        \n",
    "        for row in data_csv :\n",
    "            if i<length:\n",
    "                img_name = os.getcwd()+\"/ipeo_data/rgb/\"+row[0]+\"_rgb.tif\"\n",
    "                img_label_name = os.getcwd()+\"/ipeo_data/alpine_label/\"+row[0]+\"_label.tif\"   \n",
    "                i+=1\n",
    "        # example format: 'baseFolder/agricultural/agricultural07.tif'\n",
    "                self.data.append((\n",
    "                    img_name,\n",
    "                    img_label_name          # get index for label class\n",
    "                ))\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def __getitem__(self, x):\n",
    "        img_name, img_label_name = self.data[x]\n",
    "    \n",
    "        img = Image.open(img_name)\n",
    "        img_label = Image.open(img_label_name)\n",
    "        \n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "            img_label = self.transforms_label(img_label)\n",
    "        return img,img_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4816b2bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n",
      "dataset of length 7357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3112a3d58a494e858bfe1e865e1996",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='idx', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the dataset (call the constructor __init__)\n",
    "import matplotlib\n",
    "bounds = np.linspace(0, 7, 8)\n",
    "norm = matplotlib.colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend='both')\n",
    "\n",
    "\n",
    "dataset = Alpine(split= \"train\",frac=1.0)\n",
    "print(f\"dataset of length {len(dataset)}\")\n",
    "# plot individual samples\n",
    "from ipywidgets import interact\n",
    "@interact(idx=range(2000))\n",
    "def plot_sample(idx=0):\n",
    "    img, img_label = dataset[idx]\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(np.reshape(img_label,(200,200)), norm=norm, cmap='terrain')\n",
    "    plt.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=\"terrain\"),orientation=\"vertical\",shrink=0.6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9ca4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,1):\n",
    "    img, img_label = dataset[i]\n",
    "    water = np.reshape(img_label,(200,200)).flatten()\n",
    "    if len(water[water==1]) !=0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb99a83",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64a067af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mean and standard deviation of the dataset \n",
    "mean=torch.tensor([0.4572, 0.5079, 0.4388])\n",
    "std=torch.tensor([0.2366, 0.2141, 0.1992])\n",
    "# normalize image [0-1] (or 0-255) to zero-mean unit standard deviation\n",
    "normalize = T.Normalize(mean, std)\n",
    "# we invert normalization for plotting later\n",
    "std_inv = 1 / (std + 1e-7)\n",
    "unnormalize = T.Normalize(-mean * std_inv, std_inv)\n",
    "\n",
    "transforms_train = T.Compose([\n",
    "  #TODO: add your own transforms here\n",
    "\n",
    "  T.Resize((200, 200)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\"\"\"\n",
    "  T.RandomResizedCrop((200, 200)),\n",
    "  T.RandomGrayscale(),\n",
    "  T.RandomHorizontalFlip(),\n",
    "  T.RandomApply([T.GaussianBlur(kernel_size=7)]),\n",
    "  T.RandomPosterize(bits=8),\n",
    "  T.RandomVerticalFlip(),\n",
    "  T.ColorJitter(),\n",
    "\"\"\"\n",
    "# we do not augment the validation dataset (aside from resizing and tensor casting)\n",
    "transforms_val = T.Compose([\n",
    "  T.Resize((200, 200)),\n",
    "  T.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dfe59",
   "metadata": {},
   "source": [
    "## Model  Base on AlexNet + https://github.com/milesial/Pytorch-UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee67fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e317372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 16)\n",
    "        self.down1 = Down(16, 32)\n",
    "        self.down2 = Down(32, 64)\n",
    "        self.down3 = Down(64, 128)\n",
    "        self.down4 = Down(128, 256)\n",
    "        self.down5 = Down(256, 512)       \n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down6 = Down(512, 1024 // factor)\n",
    "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)\n",
    "        self.up5 = Up(64, 32, bilinear)\n",
    "        self.up6 = Up(32, 16, bilinear)\n",
    "        \n",
    "        self.outc = OutConv(16, n_classes) #initial size ! (MLP here )\n",
    "        ## Note : the last step == fully connected NN (MLP) || could be repplace by a Random forest methods ! (try)\n",
    "\n",
    "    def forward(self, x,RF=False):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x6 = self.down5(x5)\n",
    "        x7 = self.down6(x6)\n",
    "        \n",
    "        x = self.up1(x7, x6)\n",
    "        x = self.up2(x, x5)\n",
    "        x = self.up3(x, x4)\n",
    "        x = self.up4(x, x3)\n",
    "        x = self.up5(x, x2)\n",
    "        x = self.up6(x, x1)\n",
    "        \n",
    "        if not RF :\n",
    "            logits = self.outc(x)\n",
    "            return logits\n",
    "        else :\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157b23ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67d5bac7",
   "metadata": {},
   "source": [
    "## Getloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f49249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(split,frac=1.0):\n",
    "    \n",
    "    \n",
    "    data_dataset = Alpine(transforms=transforms_train, split=split,frac=frac)\n",
    "    shuffle = True\n",
    "    if split == \"test\":\n",
    "        shuffle = False\n",
    "    # data loader\n",
    "    data_loader = DataLoader(data_dataset, \n",
    "                              batch_size  = 2, \n",
    "                              shuffle     = shuffle, \n",
    "                              num_workers = 1,\n",
    "                              pin_memory  = False)\n",
    "    return data_dataset, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9df83458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3679"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " train_loader = get_dataloaders(\"train\")[1]\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4aa389a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_epoch(model, optimizer, scheduler, criterion, train_loader, epoch, device,RF):\n",
    "    ### SOLUTION\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    accuracy_history = []\n",
    "    lr_history = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        target = target.type(torch.LongTensor) #avoid an error idk why?\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data, RF)\n",
    "        if not RF :\n",
    "            #shape with not RF : CNN + MLP : prediction already done\n",
    "            #print(output.shape) #[batch,class,200,200]\n",
    "            #print(target.shape)#[batch,1,200,200]\n",
    "            #Newsize for criterion\n",
    "            output = output.permute(1,0,2,3).flatten(1).permute(1,0) #[batch*200*200,class]\n",
    "            target = target.flatten() #[batch*200*200]\n",
    "\n",
    "        else :\n",
    "            #shape with  RF : CNN + RF\n",
    "            #print(output.shape) #[batch,features,200,200] .. ici features = 16\n",
    "            #print(target.shape)#[batch,1,200,200]\n",
    "            #RF prediction\n",
    "            output = RF_CNN(output,target,nbr_class=8) #[batch*200*200,class]\n",
    "            target = target.flatten() #[batch*200*200]\n",
    "            print(output)\n",
    "            print(output.shape)\n",
    "            \n",
    "            \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        predictions = output.argmax(1).cpu().detach().numpy()\n",
    "        ground_truth = target.cpu().detach().numpy()\n",
    "        #correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "        #ccuracy_float = correct / len(output[0])\n",
    "        \n",
    "        accuracy_float = (predictions == ground_truth).mean()\n",
    "        loss_float = loss.item()\n",
    "\n",
    "        loss_history.append(loss_float)\n",
    "        accuracy_history.append(accuracy_float)\n",
    "        lr_history.append(scheduler.get_last_lr()[0])\n",
    "        if batch_idx % (len(train_loader.dataset) // len(data) // 10) == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch}-{batch_idx:03d} \"\n",
    "                f\"batch_loss={loss_float:0.2e} \"\n",
    "                f\"batch_acc={accuracy_float:0.3f} \"\n",
    "                f\"lr={scheduler.get_last_lr()[0]:0.3e} \"\n",
    "            )\n",
    "\n",
    "    return loss_history, accuracy_history, lr_history\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, device, val_loader, criterion,RF):\n",
    "    model.eval()  # Important: eval mode (affects dropout, batch norm etc)\n",
    "    test_loss = 0\n",
    "    accuracy_float = 0 \n",
    "    for data, target in val_loader:\n",
    "        target = target.type(torch.LongTensor) #avoid an error idk why?\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data,RF=False)\n",
    "        output = model(data, RF)\n",
    "        if not RF :\n",
    "            #shape with not RF : CNN + MLP : prediction already done\n",
    "            #print(output.shape) #[batch,class,200,200]\n",
    "            #print(target.shape)#[batch,1,200,200]\n",
    "            #Newsize for criterion\n",
    "            output = output.permute(1,0,2,3).flatten(1).permute(1,0) #[batch*200*200,class]\n",
    "            target = target.flatten() #[batch*200*200]\n",
    "        else :\n",
    "            #shape with  RF : CNN + RF\n",
    "            #print(output.shape) #[batch,features,200,200] .. ici features = 16\n",
    "            #print(target.shape)#[batch,1,200,200]\n",
    "            #RF prediction\n",
    "            output = RF_CNN(output,target,nbr_class=8) #[batch*200*200,class]\n",
    "            output = output.to(device)\n",
    "            target = target.to(device)\n",
    "            target = target.flatten() #[batch*200*200]\n",
    "        \n",
    "        test_loss += criterion(output, target)\n",
    "        \n",
    "        predictions = output.argmax(1).cpu().detach().numpy()\n",
    "        ground_truth = target.cpu().detach().numpy()\n",
    "        \n",
    "        accuracy_float += (predictions == ground_truth).mean()\n",
    "\n",
    "    test_loss /= len(val_loader)\n",
    "\n",
    "    print(\n",
    "        \"Val set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            accuracy_float,\n",
    "            len(val_loader),\n",
    "            100.0 * accuracy_float / len(val_loader),\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy_float / len(val_loader)\n",
    "\n",
    "\n",
    "def run_training(\n",
    "    model_factory,\n",
    "    num_epochs,\n",
    "    optimizer_kwargs,\n",
    "    device=\"cuda\",\n",
    "    RF=False,\n",
    "    frac=1.0,\n",
    "):\n",
    "    # ===== Data Loading =====\n",
    "    train_loader = get_dataloaders(\"train\",frac=frac)[1]\n",
    "    val_loader = get_dataloaders(\"val\",frac=frac)[1]\n",
    "\n",
    "    # ===== Model, Optimizer and Criterion =====\n",
    "    model = UNet(3,8)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), **optimizer_kwargs)\n",
    "    criterion = torch.nn.functional.cross_entropy\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=(len(train_loader.dataset) * num_epochs) // train_loader.batch_size,\n",
    "    )\n",
    "\n",
    "    # ===== Train Model =====\n",
    "    lr_history = []\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    print(\"le training commence!\")\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_acc, lrs = train_epoch(\n",
    "            model, optimizer, scheduler, criterion, train_loader, epoch, device,RF\n",
    "        )\n",
    "        train_loss_history.extend(train_loss)\n",
    "        train_acc_history.extend(train_acc)\n",
    "        lr_history.extend(lrs)\n",
    "\n",
    "        val_loss, val_acc = validate(model, device, val_loader, criterion,RF)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "    \n",
    "    #test part\n",
    "    \n",
    "    return (sum(train_acc) / len(train_acc), val_acc, model)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6407b0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' IF curve .. (not working)\\n    # ===== Plot training curves =====\\n    n_train = len(train_acc_history)\\n    t_train = num_epochs * np.arange(n_train) / n_train\\n    t_val = np.arange(1, num_epochs + 1)\\n\\n    plt.figure(figsize=(6.4 * 3, 4.8))\\n    plt.subplot(1, 3, 1)\\n    plt.plot(t_train, train_acc_history, label=\"Train\")\\n    plt.plot(t_val, val_acc_history, label=\"Val\")\\n    plt.legend()\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Accuracy\")\\n\\n    plt.subplot(1, 3, 2)\\n    plt.plot(t_train, train_loss_history, label=\"Train\")\\n    plt.plot(t_val, val_loss_history, label=\"Val\")\\n    plt.legend()\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Loss\")\\n\\n    plt.subplot(1, 3, 3)\\n    plt.plot(t_train, lr_history)\\n    plt.xlabel(\"Epoch\")\\n    plt.ylabel(\"Learning Rate\")\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" IF curve .. (not working)\n",
    "    # ===== Plot training curves =====\n",
    "    n_train = len(train_acc_history)\n",
    "    t_train = num_epochs * np.arange(n_train) / n_train\n",
    "    t_val = np.arange(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(6.4 * 3, 4.8))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(t_train, train_acc_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_acc_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(t_train, train_loss_history, label=\"Train\")\n",
    "    plt.plot(t_val, val_loss_history, label=\"Val\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(t_train, lr_history)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Learning Rate\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c18e8c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "7357\n",
      "1226\n",
      "le training commence!\n",
      "Train Epoch: 1-000 batch_loss=1.89e+00 batch_acc=0.279 lr=1.000e-03 \n",
      "Train Epoch: 1-367 batch_loss=1.14e+00 batch_acc=0.696 lr=9.755e-04 \n",
      "Train Epoch: 1-734 batch_loss=1.25e+00 batch_acc=0.527 lr=9.047e-04 \n",
      "Train Epoch: 1-1101 batch_loss=2.42e+00 batch_acc=0.010 lr=7.944e-04 \n",
      "Train Epoch: 1-1468 batch_loss=1.94e+00 batch_acc=0.092 lr=6.554e-04 \n",
      "Train Epoch: 1-1835 batch_loss=9.09e-01 batch_acc=0.613 lr=5.013e-04 \n",
      "Train Epoch: 1-2202 batch_loss=8.58e-01 batch_acc=0.845 lr=3.470e-04 \n",
      "Train Epoch: 1-2569 batch_loss=8.45e-01 batch_acc=0.853 lr=2.077e-04 \n",
      "Train Epoch: 1-2936 batch_loss=1.07e+00 batch_acc=0.740 lr=9.685e-05 \n",
      "Train Epoch: 1-3303 batch_loss=1.66e+00 batch_acc=0.402 lr=2.530e-05 \n",
      "Train Epoch: 1-3670 batch_loss=1.40e+00 batch_acc=0.384 lr=8.937e-09 \n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "Caught UnidentifiedImageError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/1165175/ipykernel_33882/3515169777.py\", line 74, in __getitem__\n    img_label = Image.open(img_label_name)\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/PIL/Image.py\", line 3187, in open\n    \"cannot identify image file %r\" % (filename if filename else fp)\nPIL.UnidentifiedImageError: cannot identify image file '/home/atexier/IPEO-project-2022/ipeo_data/alpine_label/25595_11025_label.tif'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/1165175/ipykernel_33882/1030811621.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mRF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mfrac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;32m/tmp/1165175/ipykernel_33882/3085176128.py\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model_factory, num_epochs, optimizer_kwargs, device, RF, frac)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mlr_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mval_loss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mval_acc_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipeo_venv/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/1165175/ipykernel_33882/3085176128.py\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, device, val_loader, criterion, RF)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0maccuracy_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#avoid an error idk why?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1333\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1360\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ipeo_venv/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0;31m# instantiate since we don't know how to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: Caught UnidentifiedImageError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/1165175/ipykernel_33882/3515169777.py\", line 74, in __getitem__\n    img_label = Image.open(img_label_name)\n  File \"/home/atexier/ipeo_venv/lib/python3.7/site-packages/PIL/Image.py\", line 3187, in open\n    \"cannot identify image file %r\" % (filename if filename else fp)\nPIL.UnidentifiedImageError: cannot identify image file '/home/atexier/IPEO-project-2022/ipeo_data/alpine_label/25595_11025_label.tif'\n"
     ]
    }
   ],
   "source": [
    "#from RF_from_CNN import RF_CNN\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "image_size = 400\n",
    "model_factory = UNet\n",
    "num_epochs = 1\n",
    "frac=1.0\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "optimizer_kwargs = dict(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-2,\n",
    ")\n",
    "\n",
    "train_acc,val_acc,model = run_training(\n",
    "    model_factory=UNet,\n",
    "    num_epochs=num_epochs,\n",
    "    optimizer_kwargs=optimizer_kwargs,\n",
    "    device=device,\n",
    "    RF=False,\n",
    "    frac=frac\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2ea05",
   "metadata": {},
   "source": [
    "## Testing the  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fd8a345",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/1165175/ipykernel_33882/2355735933.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eefa2758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  7 12:40:05 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                  Off |\r\n",
      "| N/A   34C    P0    24W / 250W |      4MiB / 32510MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8715b8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset,test_loader = get_dataloaders(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442e0628",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def test(model, device, test_loader, criterion):\n",
    "    test_loss = 0\n",
    "    accuracy_float = 0 \n",
    "    print(len(test_loader))\n",
    "    test_pred = []\n",
    "    test_ground_truth =[]\n",
    "    all_acc = []\n",
    "    for data, target in test_loader:\n",
    "        batch_size = len(data)\n",
    "        target = target.type(torch.LongTensor) #avoid an error idk why?\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        #print(output.shape) #[batch,class,200,200]\n",
    "        #print(target.shape)#[batch,1,200,200]\n",
    "        output = output.permute(1,0,2,3).flatten(1).permute(1,0) #[batch*200*200,class]    \n",
    "        target = target.flatten() #[batch*200*200]\n",
    "   \n",
    "        \n",
    "        test_loss += criterion(output, target)\n",
    "        \n",
    "        predictions = output.argmax(1).cpu().detach().numpy()\n",
    "        ground_truth = target.cpu().detach().numpy()\n",
    "        \n",
    "        predictions = predictions.reshape((batch_size,200,200))\n",
    "        ground_truth = ground_truth.reshape((batch_size,200,200))\n",
    "        \n",
    "        accuracy_batch = (predictions == ground_truth).mean()\n",
    "        accuracy_float += accuracy_batch\n",
    "        for i in range(batch_size):            \n",
    "            all_acc.append((predictions[i] == ground_truth[i]).mean())\n",
    "            test_pred.append(predictions[i])\n",
    "            test_ground_truth.append(ground_truth[i])\n",
    "           \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\".format(\n",
    "            test_loss,\n",
    "            accuracy_float,\n",
    "            len(test_loader),\n",
    "            100.0 * accuracy_float / len(test_loader),\n",
    "        )\n",
    "    )\n",
    "    return test_loss, accuracy_float / len(test_loader.dataset),test_pred,test_ground_truth,all_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ce1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = torch.nn.functional.cross_entropy\n",
    "test_loss, test_acc,test_pred,test_ground_truth,all_acc = test(model, device, test_loader, criterion) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59954d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print predictions\n",
    "#test_dataset\n",
    "LABEL_CLASSES = {\n",
    "      'rocks': \t\t  7,\n",
    "      'scree': \t\t\t    6,\n",
    "      'sparse_rocks': \t  5,\n",
    "      'water': \t\t\t\t      3,\n",
    "      'glacier_and_permanent_snow': \t\t\t    4,\n",
    "      'forest': \t\t\t    1,\n",
    "      'sparse_forest':   2,\n",
    "      'grasslands_and_others': \t\t\t\t    0,\n",
    "      \n",
    "    }\n",
    "    \n",
    "import matplotlib\n",
    "print(f\"dataset of length {len(test_dataset)}\")\n",
    "# plot individual samples\n",
    "bounds = np.linspace(0, 7, 8)\n",
    "norm = matplotlib.colors.BoundaryNorm(boundaries=bounds, ncolors=256, extend='both')\n",
    "\n",
    "from ipywidgets import interact\n",
    "@interact(idx=range(214))\n",
    "def plot_sample(idx=0):\n",
    "\n",
    "    img_data, img_label = test_dataset[idx][0],test_dataset[idx][1]\n",
    "    img_pred = test_pred[idx]\n",
    "    print(img_label.shape)\n",
    "    \n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.subplot(1,3,1,title=\"Real image\")\n",
    "    fig = plt.imshow(unnormalize(img_data).permute(1,2,0).cpu().detach().numpy())\n",
    "    \n",
    "    plt.subplot(1,3,2,title=\"Predict label \\n accuracy \"+str(all_acc[idx]))\n",
    "    plt.imshow(img_pred, norm=norm, cmap='terrain')\n",
    "    plt.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=\"terrain\"),orientation=\"vertical\",shrink=0.3)\n",
    "    \n",
    "    plt.subplot(1,3,3,title=\"True label\")\n",
    "    plt.imshow(np.reshape(img_label,(200,200)), norm=norm, cmap='terrain')\n",
    "    \n",
    "    plt.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=\"terrain\"),orientation=\"vertical\",shrink=0.3)\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9880ffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data, img_label = test_dataset[5][0],test_dataset[5][1]\n",
    "img_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d78971",
   "metadata": {},
   "source": [
    "# Confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a8dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(conf_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(9, 9))\n",
    "    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            ax.text(x=j, y=i,s=conf_matrix[i, j], va='center', ha='center', size='x-large')\n",
    "    \n",
    "    plt.xlabel('Predictions', fontsize=18)\n",
    "    plt.ylabel('Ground Truth', fontsize=18)\n",
    "    plt.title('Confusion Matrix', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de53541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "all_gt_labels = np.array(test_ground_truth).flatten()\n",
    "all_predictions = np.array(test_pred).flatten()\n",
    "conf_matrix = confusion_matrix(all_gt_labels, all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede65e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c118a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(all_gt_labels==2)[0]),len(np.where(all_predictions==3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae636bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_CLASSES = {\n",
    "      'rocks': \t\t  7,\n",
    "      'scree': \t\t\t    6,\n",
    "      'sparse_rocks': \t  5,\n",
    "      'water': \t\t\t\t      3,\n",
    "      'glacier_and_permanent_snow': \t\t\t    4,\n",
    "      'forest': \t\t\t    1,\n",
    "      'sparse_forest':   2,\n",
    "      'grasslands_and_others': \t\t\t\t    0,\n",
    "      \n",
    "    }\n",
    "    \n",
    "def data_information(conf_matrix):\n",
    "    fig, ax = plt.subplots(figsize=(12, 9))\n",
    "    ax.matshow(np.zeros((6,conf_matrix.shape[1])), cmap=plt.cm.Blues, alpha=1)\n",
    "    for j in range(len(conf_matrix)):\n",
    "        ax.text(x=j, y=0,s=np.round(np.sum(conf_matrix[j,:])/np.sum(conf_matrix),decimals=2), va='center', ha='center', size='x-large')\n",
    "        ax.text(x=j, y=1,s=np.round(conf_matrix[j,j]/np.sum(conf_matrix[j,:]),decimals=2), va='center', ha='center', size='x-large')\n",
    "        ax.text(x=j, y=2,s=np.round(conf_matrix[j,j]/np.sum(conf_matrix[:,j]),decimals=2), va='center', ha='center', size='x-large')\n",
    "        \n",
    "        \n",
    "    ax.text(x=0, y=4,s=np.round(np.sum(np.diag(conf_matrix))/np.sum(conf_matrix),decimals=2), va='center', ha='center', size='x-large')\n",
    "    ax.text(x=0, y=5,s=np.round(np.sum(np.diag(conf_matrix)/np.sum(conf_matrix,axis=1))/len(conf_matrix),decimals=2), va='center', ha='center', size='x-large')\n",
    "    \n",
    "    ax.set_yticklabels(['',\"Pixel_class\", \"Producer_accuracy\",\"User_accuracy\", \"\",\"Overall_accuracy\", \"Average_accuracy\"])\n",
    "    ax.set_xticklabels(['',\"grasslands_and_others\",\"forest\",\"sparse_forest\",\"water\",\"glacier/snow\",\"sparse_rock\",\"scree\",\"rocks\"])\n",
    "\n",
    "\n",
    "    plt.title('data_information', fontsize=18)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e130f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_information(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09d0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_acc = np.sum(np.diag(conf_matrix))/np.sum(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853732f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(conf_matrix,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708cb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58f137a",
   "metadata": {},
   "source": [
    "## Number of pixels per class in the entire training set (no need to run)\n",
    "Answer in commentary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No need to rerun!\")\n",
    "raise ValueError('No need to rerun!')\n",
    "\n",
    "img, img_label = dataset[0]\n",
    "img, img_label = np.array(img), np.array(img_label)\n",
    "\n",
    "num_pix_grasslands_and_others = 0\n",
    "num_pix_forest = 0\n",
    "num_pix_sparse_forest = 0\n",
    "num_pix_water = 0\n",
    "num_pix_glacier_and_permanent_snow = 0\n",
    "num_pix_sparse_rocks = 0\n",
    "num_pix_scree = 0\n",
    "num_pix_rocks = 0\n",
    "num_pix_misclassified = 0\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    img, img_label = dataset[i]\n",
    "    img, img_label = np.array(img), np.array(img_label)\n",
    "    img_label = img_label.flatten()\n",
    "    for j in range(img_label.shape[0]):\n",
    "        if img_label[j] == 0:\n",
    "            num_pix_grasslands_and_others += 1\n",
    "        elif img_label[j] == 1 :\n",
    "            num_pix_forest += 1\n",
    "        elif img_label[j] == 2 :\n",
    "            num_pix_sparse_forest += 1\n",
    "        elif img_label[j] == 3 :\n",
    "            num_pix_water += 1\n",
    "        elif img_label[j] == 4 :\n",
    "            num_pix_glacier_and_permanent_snow += 1\n",
    "        elif img_label[j] == 5 :\n",
    "            num_pix_sparse_rocks += 1\n",
    "        elif img_label[j] == 6 :\n",
    "            num_pix_scree += 1\n",
    "        elif img_label[j] == 7 :\n",
    "            num_pix_rocks += 1\n",
    "        else:\n",
    "            num_pix_misclassified += 1\n",
    "            \n",
    "print(f\"Number of pixels of grasslands and others : {num_pix_grasslands_and_others}\")\n",
    "print(f\"Number of pixels of forest : {num_pix_forest}\")\n",
    "print(f\"Number of pixels of sparse forest : {num_pix_sparse_forest}\")\n",
    "print(f\"Number of pixels of water : {num_pix_water}\")\n",
    "print(f\"Number of pixels of glacier and permanent snow : {num_pix_glacier_and_permanent_snow}\")\n",
    "print(f\"Number of pixels of sparse rocks : {num_pix_sparse_rocks}\")\n",
    "print(f\"Number of pixels of scree : {num_pix_scree}\")\n",
    "print(f\"Number of pixels of rocks : {num_pix_rocks}\")\n",
    "print(f\"Number of pixels missclassified : {num_pix_misclassified}\")\n",
    "\n",
    "\"\"\"\n",
    "ANSWER:\n",
    "Number of pixels of grasslands and others : 72519725\n",
    "Number of pixels of forest : 19336033\n",
    "Number of pixels of sparse forest : 18342686\n",
    "Number of pixels of water : 5936946\n",
    "Number of pixels of glacier and permanent snow : 10797402\n",
    "Number of pixels of sparse rocks : 16857098\n",
    "Number of pixels of scree : 78876458\n",
    "Number of pixels of rocks : 71613652\n",
    "Number of pixels missclassified : 0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c285e9",
   "metadata": {},
   "source": [
    "## Test ( Image input 400 sur 400 / label 200 sur 200 --> need to resize the input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae0a2d",
   "metadata": {},
   "source": [
    "### Find the mean and stf of the Dataset || No need run  anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cc6ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = T.Compose([\n",
    "  T.ToTensor()\n",
    "])\n",
    "\n",
    "# dataset\n",
    "train_dataset = Alpine(transforms=transforms_train, split='train')\n",
    "test_dataset = Alpine(transforms=transforms_train, split='test')\n",
    "val_dataset = Alpine(transforms=transforms_train, split='val')\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size  = 16, \n",
    "                          shuffle     = False, \n",
    "                          num_workers = 1,\n",
    "                          pin_memory  = True)\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size  = 16, \n",
    "                          shuffle     = False, \n",
    "                          num_workers = 1,\n",
    "                          pin_memory  = True)\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size  = 16, \n",
    "                          shuffle     = False, \n",
    "                          num_workers = 1,\n",
    "                          pin_memory  = True)\n",
    "\n",
    "def mean_std():\n",
    "    ####### COMPUTE MEAN / STD\n",
    "\n",
    "    # placeholders\n",
    "    psum    = torch.tensor([0.0, 0.0, 0.0])\n",
    "    psum_sq = torch.tensor([0.0, 0.0, 0.0])\n",
    "\n",
    "    #The first two steps are done in the snippet below. Note that we set axis = [0, 2, 3] \n",
    "    #to compute mean values with respect to axis 1. The dimensions of inputs is [batch_size x 3 x image_size x image_size],\n",
    "    #so we need to make sure we aggregate values per each RGB channel separately.\n",
    "\n",
    "    # loop through images\n",
    "    for img,img_label in tqdm(train_loader):\n",
    "        psum    += img.sum(axis        = [0, 2, 3])\n",
    "        psum_sq += (img ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "    for img,img_label in tqdm(test_loader):\n",
    "        psum    += img.sum(axis        = [0, 2, 3])\n",
    "        psum_sq += (img ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "    for img,img_label in tqdm(val_loader):\n",
    "        psum    += img.sum(axis        = [0, 2, 3])\n",
    "        psum_sq += (img ** 2).sum(axis = [0, 2, 3])\n",
    "\n",
    "    ####### FINAL CALCULATIONS\n",
    "\n",
    "    # image count\n",
    "    s = 0\n",
    "    for image in test_csv :\n",
    "        s+=1\n",
    "    for image in train_csv :\n",
    "        s+=1\n",
    "    for image in val_csv :\n",
    "        s+=1\n",
    "    # pixel count\n",
    "    image_size = train_dataset[0][0].shape #[3,400,400]     \n",
    "    count = s * image_size[1] * image_size[1]\n",
    "\n",
    "    # mean and std\n",
    "    total_mean = psum / count\n",
    "    total_var  = (psum_sq / count) - (total_mean ** 2)\n",
    "    total_std  = torch.sqrt(total_var)\n",
    "\n",
    "    # output\n",
    "    print('mean: '  + str(total_mean))\n",
    "    print('std:  '  + str(total_std))\n",
    "    return total_mean,total_std\n",
    "# mean,std = mean_std()\n",
    "\"\"\"\n",
    "GOT: \n",
    "mean: tensor([0.4572, 0.5079, 0.4388])\n",
    "std:  tensor([0.2366, 0.2141, 0.1992])\n",
    "Use in normalisze transforms\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
